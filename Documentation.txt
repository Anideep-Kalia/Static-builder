AIM: This projects make containers on demand in which an application on GitHub is running and these container will export the built project to S3 bucket which will stream this project so the project is static website

BUILD SERVER
-> made the docker file and the end run the shell script
-> made the shell file which will clone the repo and run script.js
-> Made the script.js => have init function
. will run the npm i & npm run dev
. store all the logs 
. and storing every scipt in s3 bucket except the folders => GPT
-> Made a bucket on s3 => made it public => add security policy 
-> Create IAM user granting admin powers => create new access key for this => cli => description: any => got access key and shove it in script.js
-> made a ECR (private) => view push command
-> Made a ECS => fargate => Made a target group with ECR image and everything at default
-> run the image on local machine 
-> Now testing if the ECS is working => Run new task 
. compute config : launch type
. fargate
. family: {target group made}
. container overrides => builder-image => key-value{put in env data i.e. GitHub repo url and project_id: p1} || project_id is the actually file/project name in which project is made in s3 and it will be different for different containers
. create
-> Now we will attach the redis server on aiven
. inside publisher we will in the serviceURI in plaintext inside aiven redis server
. now we have made a function publishLog(log) inside which we are publishing the messages taking as params on channel 'logs:${PROJECT_ID}'
-> now again push the image in ECR by view push commands
-> now we have made socket.io connection inside api-server


S3-REVERSE-PROXY
-> index.js: in this file we have made a reverse proxy which will redirect every call from 
 => resolvesTo: {copy path from bucket and at the end it will be (p1/index.html)=> change this from where the request is coming}
-> after making the reverse proxy we will test it by running: a1.localhost:8000/index.html => remember that we first have to try our docker image with project_id a1 then only it will run
-> now we have a problem that we have to write index.html at end which is handled by the proxy.on('proxyreq',(proxy,req,res)=>
. so what it will do it will route all the request ending with '/' to index.html

API-SERVER
-> Now this will automate the generation of the containers 
-> installed aws-sdk/client-ecs:  is a package in the AWS SDK for JavaScript, specifically for Node.js and the browser, which provides a client for interacting with Amazon Elastic Container Service (ECS)
-> now made a config const in which we have filled cluster arn and task definition arn
-> in const command inside networkConfiguration give subnets of clusters and security group of cluster
-> inside overrides give name of image(take from task definition json)
-> now to test send a post request like in image
-> Now streming logs to user
. made a subscriber like in publisher from build-server index.js
. io is listening on port 9001
. now made io.on(connection) with channel 
. then we have made function initRediSubscribe() which will establish connection between redis and socket.io
. inside above function we have made a pattern subscribe i.e. all the channels starting with logs: will be considered and from these channel we will find take project_id as channel and message as logs
-> now to execute go to execution part
-> now we have modified the code so if the user have slug then he can give slug and can make project on the specific url we have made the changes on line 40

LOGGS
-> made a server of redis on aiven
-> all the code for redis and websocket in script.js inside build-server


EXECUTION
-> On postman connect to localhost:9002 and in events turn on the message
. now in message tab => logs: {projectslug_from_9000/project}
. write: subscribe(in input field) and send

-> connect to localhost:9000/project
>inside raw:
. give "gitURL": "{githhub url}"
. give "slug": "{write slug if you have"
. now you will get projectSlug

